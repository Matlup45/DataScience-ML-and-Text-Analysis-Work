{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHcsiUz6fgEXydlD3Kwl5v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matlup45/Text-Analysis/blob/main/Text_Extraction_%26_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting the data from given URLs**"
      ],
      "metadata": {
        "id": "_TJFY6fPZ7ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_article_content(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Find the parent container of the article content\n",
        "        parent_container = soup.find('article')\n",
        "\n",
        "        # Extract the article title from the parent container\n",
        "        title_element = parent_container.find('h1')\n",
        "        title = title_element.get_text().strip() if title_element else None\n",
        "\n",
        "        # Extract the article text from the parent container\n",
        "        text_elements = parent_container.find_all('p')\n",
        "        text = ' '.join([element.get_text().strip() for element in text_elements])\n",
        "\n",
        "        return title, text\n",
        "    else:\n",
        "        # If the request was unsuccessful, print the status code\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return None, None\n",
        "\n",
        "data = pd.read_excel('urls.xlsx')\n",
        "\n",
        "# Iterate over the rows in the Excel file\n",
        "for index, row in data.iterrows():\n",
        "    file_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    # Extracting the article content\n",
        "    title, text = extract_article_content(url)\n",
        "\n",
        "    if title and text:\n",
        "        file_name = f\"{file_id}.txt\"\n",
        "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(f\"Title: {title}\\n\")\n",
        "            file.write(f\"Text: {text}\\n\")\n",
        "\n",
        "        print(f\"Extraction successful. The text for File ID '{file_id}' has been saved to {file_name}.\")\n",
        "    else:\n",
        "        print(f\"Extraction failed for File ID '{file_id}'.\")"
      ],
      "metadata": {
        "id": "FUiHCntHa7u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Analysis the data**"
      ],
      "metadata": {
        "id": "lLGhBTRIaJro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Folder path containing the text files\n",
        "folder_path = r'Folder location'\n",
        "\n",
        "# Initializing an empty list to store the analysis results\n",
        "results = []\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "\n",
        "        # Reading the text file\n",
        "\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Tokenize the text into sentences\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        # Split the text into words\n",
        "        words = nltk.word_tokenize(text)\n",
        "\n",
        "        # Calculate average sentence length\n",
        "        num_sentences = len(sentences)\n",
        "        num_words = len(words)\n",
        "        average_sentence_length = num_words / num_sentences\n",
        "\n",
        "        # Define a function to check if a word is complex\n",
        "        def is_complex_word(word):\n",
        "            # Considering criteria for a complex word\n",
        "            return len(word) >= 6\n",
        "\n",
        "        # Count the number of complex words\n",
        "        num_complex_words = sum(1 for word in words if is_complex_word(word))\n",
        "\n",
        "        # Calculating the percentage of complex words\n",
        "        percentage_complex_words = (num_complex_words / num_words) * 100\n",
        "\n",
        "        # Calculating the Gunning Fog index\n",
        "        fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "        # Store the analysis results for the current text file\n",
        "        results.append({\n",
        "            'File': filename,\n",
        "            'Average Sentence Length': average_sentence_length,\n",
        "            'Percentage of Complex Words': percentage_complex_words,\n",
        "            'Fog Index': fog_index\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Sav the DataFrame to an Excel file\n",
        "output_file_path = 'readability_analysis.xlsx'\n",
        "df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Readability analysis saved to '{output_file_path}'\")"
      ],
      "metadata": {
        "id": "rU1W9ER0bEii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 3: calculating Average Number of Words Per Sentence**"
      ],
      "metadata": {
        "id": "vOELTkVjaTBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "folder_path = r'C:\\Users\\91976\\Blackcoffer_assignment'\n",
        "\n",
        "results = []\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Reading the text file\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Tokenize the text into sentences\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        # Split the text into words\n",
        "        words = nltk.word_tokenize(text)\n",
        "\n",
        "        # Calculate average number of words per sentence\n",
        "        num_sentences = len(sentences)\n",
        "        num_words = len(words)\n",
        "        average_words_per_sentence = num_words / num_sentences\n",
        "\n",
        "        # Store the analysis results for the current text file\n",
        "        results.append({\n",
        "            'File': filename,\n",
        "            'Average Words Per Sentence': average_words_per_sentence\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Saving the DataFrame to an Excel file\n",
        "output_file_path = 'average_words_per_sentence.xlsx'\n",
        "df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Average words per sentence analysis saved to '{output_file_path}'\")"
      ],
      "metadata": {
        "id": "vzaLmfZWbfCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 4: calculating Syllable Count Per Word**"
      ],
      "metadata": {
        "id": "v0zmyKLnaJ4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "folder_path = r'C:\\Users\\91976\\Blackcoffer_assignment'\n",
        "\n",
        "# Initialize a variable to store the total count of syllables\n",
        "total_syllable_count = 0\n",
        "\n",
        "results = []\n",
        "\n",
        "# Function to count the number of syllables in a word\n",
        "def count_syllables(word):\n",
        "    vowels = 'aeiou'\n",
        "    exceptions = ['es', 'ed']\n",
        "\n",
        "    # Remove trailing \"es\" and \"ed\" from the word\n",
        "    for exception in exceptions:\n",
        "        if word.endswith(exception):\n",
        "            word = word[:-len(exception)]\n",
        "            break\n",
        "\n",
        "    # Count the number of vowels\n",
        "    syllable_count = 0\n",
        "    prev_char_vowel = False\n",
        "\n",
        "    for char in word:\n",
        "        if char.lower() in vowels:\n",
        "            if not prev_char_vowel:\n",
        "                syllable_count += 1\n",
        "            prev_char_vowel = True\n",
        "        else:\n",
        "            prev_char_vowel = False\n",
        "\n",
        "    return syllable_count\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Reading the text file\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Tokenize the text into words\n",
        "        words = text.split()\n",
        "\n",
        "        # Calculate the number of syllables for each word\n",
        "        syllable_counts = [count_syllables(word) for word in words]\n",
        "\n",
        "        # Calculate the total count of syllables\n",
        "        total_syllable_count += sum(syllable_counts)\n",
        "\n",
        "        # Store the analysis results for the current text file\n",
        "        file_results = {\n",
        "            'File': filename,\n",
        "            'Total Syllable Count': sum(syllable_counts)\n",
        "        }\n",
        "        results.append(file_results)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Saving the DataFrame to an Excel file\n",
        "output_file_path = 'syllable_count.xlsx'\n",
        "df.to_excel(output_file_path, index=False)\n",
        "\n",
        "# Print the total count of syllables\n",
        "print(f\"Total count of syllables: {total_syllable_count}\")\n",
        "print(f\"Syllable count analysis saved to '{output_file_path}'\")"
      ],
      "metadata": {
        "id": "q4PSYNVPbnlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 5: calculating Complex Word Count**"
      ],
      "metadata": {
        "id": "SLTU498jaKEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "# Function to count the number of syllables in a word\n",
        "def count_syllables(word):\n",
        "    vowels = 'aeiou'\n",
        "    exceptions = ['es', 'ed']\n",
        "\n",
        "    # Remove trailing \"es\" and \"ed\" from the word\n",
        "    for exception in exceptions:\n",
        "        if word.endswith(exception):\n",
        "            word = word[:-len(exception)]\n",
        "            break\n",
        "\n",
        "    # Count the number of vowels\n",
        "    syllable_count = 0\n",
        "    prev_char_vowel = False\n",
        "\n",
        "    for char in word:\n",
        "        if char.lower() in vowels:\n",
        "            if not prev_char_vowel:\n",
        "                syllable_count += 1\n",
        "            prev_char_vowel = True\n",
        "        else:\n",
        "            prev_char_vowel = False\n",
        "\n",
        "    return syllable_count\n",
        "\n",
        "# Folder path containing the text files\n",
        "folder_path = r'C:\\Users\\91976\\Blackcoffer_Assignment'\n",
        "\n",
        "# Set of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Create a list to store the results\n",
        "results = []\n",
        "\n",
        "# Process each text file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Read the text file\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Tokenize the text into words\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        # Remove punctuation from each word and convert to lowercase\n",
        "        cleaned_words = [word.lower().translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
        "\n",
        "        # Remove stop words\n",
        "        cleaned_words = [word for word in cleaned_words if word not in stop_words]\n",
        "\n",
        "        # Find complex words with more than two syllables\n",
        "        complex_words = [word for word in cleaned_words if count_syllables(word) > 2]\n",
        "\n",
        "        # Count the number of complex words\n",
        "        complex_word_count = len(complex_words)\n",
        "\n",
        "        # Append the results to the list\n",
        "        results.append({'Filename': filename, 'Complex Word Count': complex_word_count})\n",
        "\n",
        "# Convert the results to a DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "output_file = 'complex_word_count.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Complex word count for each file saved to {output_file}\")"
      ],
      "metadata": {
        "id": "Ja1hmxMEb8Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 6: calculating Word Count**"
      ],
      "metadata": {
        "id": "AQg5B27UaKVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "\n",
        "folder_path = r'C:\\Users\\91976\\Blackcoffer_assignment'\n",
        "\n",
        "# Initialize a variable to store the total count of cleaned words\n",
        "total_cleaned_word_count = 0\n",
        "\n",
        "# Set of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "results = []\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Reading the text file\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Tokenize the text into words\n",
        "        words = text.split()\n",
        "\n",
        "        # Removing punctuation from each word and convert to lowercase\n",
        "        cleaned_words = [word.lower().translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
        "\n",
        "        # Removing stop words\n",
        "        cleaned_words = [word for word in cleaned_words if word not in stop_words]\n",
        "\n",
        "        # Counting the total number of cleaned words\n",
        "        total_cleaned_word_count += len(cleaned_words)\n",
        "\n",
        "        # Store the analysis results for the current text file\n",
        "        file_results = {\n",
        "            'File': filename,\n",
        "            'Cleaned Word Count': len(cleaned_words)\n",
        "        }\n",
        "        results.append(file_results)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Saving the DataFrame to an Excel file\n",
        "output_file_path = 'cleaned_word_count.xlsx'\n",
        "df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Total count of cleaned words: {total_cleaned_word_count}\")\n",
        "print(f\"Cleaned word count analysis saved to '{output_file_path}'\")"
      ],
      "metadata": {
        "id": "fPm9gZ8zcCw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 7: calculating Personal Pronouns**"
      ],
      "metadata": {
        "id": "BvHtM0RPaLUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "folder_path = r'C:\\Users\\91976\\Blackcoffer_assignment'\n",
        "\n",
        "results = []\n",
        "\n",
        "# Define the personal pronouns pattern using regex\n",
        "pronouns_pattern = r'\\b(I|we|my|ours|us)\\b(?!\\s*US\\b)'\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Reading the text file\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Find all matches of personal pronouns in the text\n",
        "        matches = re.findall(pronouns_pattern, text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Count the number of personal pronouns\n",
        "        personal_pronoun_count = len(matches)\n",
        "\n",
        "        # Store the analysis results for the current text file\n",
        "        results.append({\n",
        "            'File': filename,\n",
        "            'Personal Pronoun Count': personal_pronoun_count\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Saving the DataFrame to an Excel file\n",
        "output_file_path = 'personal_pronoun_count.xlsx'\n",
        "df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Personal pronoun count analysis saved to '{output_file_path}'\")"
      ],
      "metadata": {
        "id": "L5MkTMQLcI4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 8: calculating Average Word Length**"
      ],
      "metadata": {
        "id": "IaZoShNnaLgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "folder_path = r'C:\\Users\\91976\\Blackcoffer_assignment'\n",
        "\n",
        "results = []\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Reading the text file\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Tokenize the text into words\n",
        "        words = text.split()\n",
        "\n",
        "        # Calculate the total number of words\n",
        "        total_words = len(words)\n",
        "\n",
        "        # Calculate the sum of the total number of characters in each word\n",
        "        total_characters = sum(len(word) for word in words)\n",
        "\n",
        "        # Calculate the average word length\n",
        "        average_word_length = total_characters / total_words\n",
        "\n",
        "        # Store the analysis results for the current text file\n",
        "        results.append({\n",
        "            'File': filename,\n",
        "            'Average Word Length': average_word_length\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Saving the DataFrame to an Excel file\n",
        "output_file_path = 'average_word_length.xlsx'\n",
        "df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Average word length analysis saved to '{output_file_path}'\")"
      ],
      "metadata": {
        "id": "MDniEa0BcgJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 9: Sentimental Analysis**"
      ],
      "metadata": {
        "id": "A7NIXf60a3qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.1: \tCleaning the data**"
      ],
      "metadata": {
        "id": "T36-vFsEcjYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "\n",
        "#Instead of for loop you can give the path of entire extract file location by \"input_file_path = r'folder_location'\n",
        "\n",
        "# Path to the input text file\n",
        "for i in range(37,151):\n",
        "    input_file_path = f'{i}''.txt'\n",
        "\n",
        "\n",
        "    # Location to the stopwords file\n",
        "    stopwords_file_path = 'stopwords.txt'\n",
        "\n",
        "    # Reading the input text file\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Tokenize the text into individual words\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Reading the stopwords from the file\n",
        "    with open(stopwords_file_path, 'r', encoding='utf-8') as file:\n",
        "        stopwords = file.read().splitlines()\n",
        "\n",
        "    # Removeing stopwords from the tokens\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "\n",
        "    # Join the filtered tokens back into a single string\n",
        "    cleaned_text = ' '.join(filtered_tokens)\n",
        "\n",
        "    # Rewriting the input text file with the cleaned text\n",
        "    with open(input_file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(cleaned_text)\n",
        "\n",
        "    print(\"Text file cleaned and rewritten successfully!\")"
      ],
      "metadata": {
        "id": "yWKzBILPaAZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.2 : \tExtracting derived variables"
      ],
      "metadata": {
        "id": "rEQjkwV4cwg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    input_file_path = 'file.txt'\n",
        "\n",
        "    # Path to the positive words file\n",
        "    positive_words_file = 'positive-words.txt'\n",
        "\n",
        "    # Path to the negative words file\n",
        "    negative_words_file = 'negative-words.txt'\n",
        "\n",
        "    positive_words = [] \t#Initializing Positive Word\n",
        "    negative_words = []\t\t#Initializing Negative Word\n",
        "\n",
        "    # Reading words from positive word file\n",
        "    with open(positive_words_file, 'r', encoding='latin-1') as file:\n",
        "        positive_words = file.read().splitlines()\n",
        "\n",
        "    # Reading words from negative word file\n",
        "    with open(negative_words_file, 'r', encoding='latin-1') as file:\n",
        "        negative_words = file.read().splitlines()\n",
        "\n",
        "    # Reading input text file\n",
        "    with open(input_file_path, 'r', encoding='latin-1') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Initialize counters\n",
        "    positive_count = 0\n",
        "    negative_count = 0\n",
        "\n",
        "    for word in text.split():\n",
        "        if word.lower() in positive_words:\n",
        "            positive_count += 1\n",
        "        elif word.lower() in negative_words:\n",
        "            negative_count += 1\n",
        "\n",
        "    numerator = positive_count - negative_count\n",
        "    denominator = positive_count + negative_count\n",
        "\n",
        "    Polarity_Score = (numerator / denominator) + 0.000001\n",
        "\n",
        "    Subjectivity_Score = denominator / 1232 + 0.000001\n",
        "\n",
        "    # Create a dictionary of positive and negative word counts\n",
        "    word_counts = {'Positive': positive_count, 'Negative': negative_count}\n",
        "\n",
        "    # Print the word counts\n",
        "    print(f'{i}''Word Counts:')\n",
        "    for word, count in word_counts.items():\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "    print(Polarity_Score)\n",
        "    print(Subjectivity_Score)\n"
      ],
      "metadata": {
        "id": "nAl9XjXgaAjd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}